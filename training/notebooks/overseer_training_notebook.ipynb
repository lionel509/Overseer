{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f008526",
   "metadata": {},
   "source": [
    "# Overseer Gemma 3n Model Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive training pipeline for the Overseer system using Google's Gemma 3n model. It includes:\n",
    "\n",
    "- **Data Preparation**: Loading and preprocessing training data from multiple sources\n",
    "- **Model Configuration**: Setting up the Gemma model with custom parameters\n",
    "- **Fine-tuning**: Training the model on system administration and AI assistant tasks\n",
    "- **Evaluation**: Testing model performance and generating metrics\n",
    "- **Continuous Learning**: Integration with user feedback for model improvement\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1. Valid Hugging Face token with access to Gemma models\n",
    "2. Kaggle API credentials for data collection\n",
    "3. Required Python packages installed (see requirements.txt)\n",
    "4. Sufficient GPU memory (recommended: 16GB+ VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51385682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# ML and DL libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Transformers and datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Custom modules (make sure the training directory is in path)\n",
    "sys.path.append('/Users/lionelweng/Downloads/Overseer/training')\n",
    "from training_config import TrainingConfig\n",
    "from data_preparation import SystemCommandsDataset\n",
    "from fine_tuning import OverseerTrainer\n",
    "from evaluation import ModelEvaluator\n",
    "from continuous_learning import ContinuousLearningManager\n",
    "from kaggle_data_collector import KaggleDataCollector\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='/Users/lionelweng/Downloads/Overseer/.env')\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers version: {torch.__version__}\")\n",
    "print(f\"üíæ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"üí∞ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602517bc",
   "metadata": {},
   "source": [
    "## 1. Training Configuration Setup\n",
    "\n",
    "Let's initialize our training configuration with all the parameters needed for fine-tuning the Gemma model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83810676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Training Configuration\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Display current configuration\n",
    "print(\"üîß Training Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Base Model: {config.base_model}\")\n",
    "print(f\"Learning Rate: {config.learning_rate}\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Max Sequence Length: {config.max_sequence_length}\")\n",
    "print(f\"Use GPU: {config.use_gpu}\")\n",
    "print(f\"Mixed Precision: {config.mixed_precision}\")\n",
    "print(f\"Output Directory: {config.output_dir}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {config.output_dir}\")\n",
    "\n",
    "# Check environment variables\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "kaggle_username = os.getenv('KAGGLE_USERNAME')\n",
    "kaggle_key = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "print(\"\\nüîê Environment Check:\")\n",
    "print(f\"HF Token: {'‚úÖ Set' if hf_token else '‚ùå Missing'}\")\n",
    "print(f\"Kaggle Username: {'‚úÖ Set' if kaggle_username else '‚ùå Missing'}\")\n",
    "print(f\"Kaggle Key: {'‚úÖ Set' if kaggle_key else '‚ùå Missing'}\")\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: HF_TOKEN not found. You'll need this to access Gemma models.\")\n",
    "    print(\"Set it in your .env file: HF_TOKEN=your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44033610",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Now let's prepare our training data by collecting datasets from Kaggle and generating synthetic examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970228cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data components\n",
    "print(\"üóÇÔ∏è  Initializing data collection components...\")\n",
    "kaggle_collector = KaggleDataCollector()\n",
    "dataset_processor = SystemCommandsDataset()\n",
    "\n",
    "# Option to download fresh data from Kaggle\n",
    "download_fresh_data = True  # Set to False to skip Kaggle download\n",
    "\n",
    "if download_fresh_data and kaggle_username and kaggle_key:\n",
    "    print(\"\\nüì• Downloading fresh data from Kaggle...\")\n",
    "    try:\n",
    "        dataset_processor.load_kaggle_data(kaggle_collector)\n",
    "        print(\"‚úÖ Kaggle data download completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not download Kaggle data: {e}\")\n",
    "        print(\"Proceeding with synthetic data only...\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping Kaggle download - using existing/synthetic data only\")\n",
    "\n",
    "# Generate training examples\n",
    "print(\"\\nüîÑ Creating training examples...\")\n",
    "training_examples = dataset_processor.create_training_examples()\n",
    "synthetic_examples = dataset_processor.generate_synthetic_data()\n",
    "\n",
    "# Combine all training data\n",
    "all_training_data = training_examples + synthetic_examples\n",
    "print(f\"üìä Total training examples: {len(all_training_data)}\")\n",
    "print(f\"   - From datasets: {len(training_examples)}\")\n",
    "print(f\"   - Synthetic: {len(synthetic_examples)}\")\n",
    "\n",
    "# Display some sample training examples\n",
    "print(\"\\nüìã Sample Training Examples:\")\n",
    "print(\"=\" * 60)\n",
    "for i, example in enumerate(all_training_data[:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input: {example['input'][:100]}...\")\n",
    "    print(f\"Output: {example['output'][:100]}...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create train/validation/test splits\n",
    "print(f\"\\nüîÄ Creating data splits...\")\n",
    "train_size = int(len(all_training_data) * config.train_split)\n",
    "val_size = int(len(all_training_data) * config.val_split)\n",
    "\n",
    "train_data = all_training_data[:train_size]\n",
    "val_data = all_training_data[train_size:train_size + val_size]\n",
    "test_data = all_training_data[train_size + val_size:]\n",
    "\n",
    "print(f\"üìà Data Split Summary:\")\n",
    "print(f\"   - Training: {len(train_data)} examples ({config.train_split*100}%)\")\n",
    "print(f\"   - Validation: {len(val_data)} examples ({config.val_split*100}%)\")\n",
    "print(f\"   - Test: {len(test_data)} examples ({config.test_split*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5ab14",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Setup\n",
    "\n",
    "Let's load the Gemma model and configure it for our training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed264381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer with our configuration\n",
    "print(\"ü§ñ Initializing Overseer Trainer...\")\n",
    "try:\n",
    "    trainer = OverseerTrainer(config)\n",
    "    print(\"‚úÖ Trainer initialized successfully!\")\n",
    "    print(f\"üìÑ Tokenizer vocab size: {len(trainer.tokenizer)}\")\n",
    "    print(f\"üß† Model parameters: {trainer.model.num_parameters():,}\")\n",
    "    \n",
    "    # Display model info\n",
    "    print(f\"\\nüîç Model Details:\")\n",
    "    print(f\"   - Architecture: {trainer.model.config.architectures}\")\n",
    "    print(f\"   - Hidden size: {trainer.model.config.hidden_size}\")\n",
    "    print(f\"   - Attention heads: {trainer.model.config.num_attention_heads}\")\n",
    "    print(f\"   - Layers: {trainer.model.config.num_hidden_layers}\")\n",
    "    print(f\"   - Vocab size: {trainer.model.config.vocab_size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing trainer: {e}\")\n",
    "    print(\"Check your HF_TOKEN and model access permissions.\")\n",
    "    raise\n",
    "\n",
    "# Prepare datasets for training\n",
    "print(f\"\\nüîÑ Preparing datasets...\")\n",
    "train_dataset = trainer.prepare_dataset(train_data)\n",
    "val_dataset = trainer.prepare_dataset(val_data)\n",
    "\n",
    "print(f\"‚úÖ Datasets prepared:\")\n",
    "print(f\"   - Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"   - Validation dataset: {len(val_dataset)} examples\")\n",
    "\n",
    "# Display tokenized example\n",
    "print(f\"\\nüî§ Sample Tokenized Input:\")\n",
    "sample_idx = 0\n",
    "sample_input = train_dataset[sample_idx]['input_ids']\n",
    "print(f\"Token IDs: {sample_input[:20]}...\")\n",
    "print(f\"Decoded: {trainer.tokenizer.decode(sample_input[:20])}...\")\n",
    "print(f\"Full length: {len(sample_input)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303d185",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "\n",
    "Now let's start the actual training process. This will fine-tune the Gemma model on our system administration and AI assistant tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c271cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training monitoring\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ Starting model training...\")\n",
    "print(f\"üìÖ Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è  Estimated training time: ~{config.num_epochs * len(train_dataset) // config.batch_size // 60} minutes\")\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Call the training method from our trainer\n",
    "    training_results = trainer.train(train_dataset, val_dataset)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "    print(f\"‚è±Ô∏è  Total training time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"üìä Final training loss: {training_results.get('train_loss', 'N/A')}\")\n",
    "    print(f\"üìä Final validation loss: {training_results.get('eval_loss', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    print(\"This might be due to insufficient GPU memory or other issues.\")\n",
    "    print(\"Try reducing batch_size or max_sequence_length in the config.\")\n",
    "    raise\n",
    "\n",
    "# Memory cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    current_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    max_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"\\nüíæ GPU Memory Usage:\")\n",
    "    print(f\"   - Current: {current_memory:.1f} GB\")\n",
    "    print(f\"   - Peak: {max_memory:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74876332",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Let's evaluate our trained model's performance on various tasks and generate some test outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dcd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "print(\"üìä Evaluating model on test dataset...\")\n",
    "\n",
    "if len(test_data) > 0:\n",
    "    test_dataset = trainer.prepare_dataset(test_data)\n",
    "    \n",
    "    # Generate some predictions\n",
    "    print(f\"\\nüîÆ Generating test predictions...\")\n",
    "    test_examples = test_data[:5]  # Take first 5 test examples\n",
    "    \n",
    "    for i, example in enumerate(test_examples):\n",
    "        print(f\"\\n--- Test Example {i+1} ---\")\n",
    "        print(f\"Input: {example['input']}\")\n",
    "        print(f\"Expected: {example['output'][:100]}...\")\n",
    "        \n",
    "        # Generate prediction using the model\n",
    "        try:\n",
    "            input_text = f\"User: {example['input']}\\nAssistant:\"\n",
    "            inputs = trainer.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            \n",
    "            if config.use_gpu and torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = trainer.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=trainer.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            prediction = trainer.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Extract just the assistant's response\n",
    "            if \"Assistant:\" in prediction:\n",
    "                prediction = prediction.split(\"Assistant:\")[-1].strip()\n",
    "            \n",
    "            print(f\"Generated: {prediction[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating prediction: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No test data available for evaluation\")\n",
    "\n",
    "# Create evaluation metrics\n",
    "print(f\"\\nüìà Model Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Training completed successfully\")\n",
    "print(f\"üìä Training examples processed: {len(train_data)}\")\n",
    "print(f\"üìä Validation examples: {len(val_data)}\")\n",
    "print(f\"üìä Test examples: {len(test_data)}\")\n",
    "print(f\"‚ö° Model size: {trainer.model.num_parameters():,} parameters\")\n",
    "print(f\"üíæ Output saved to: {config.output_dir}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb428146",
   "metadata": {},
   "source": [
    "## 6. Interactive Model Testing\n",
    "\n",
    "Test your trained model with custom prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_interactive(prompt, max_tokens=200, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Test the trained model with a custom prompt\n",
    "    \"\"\"\n",
    "    print(f\"ü§ñ Testing prompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Format the input\n",
    "        input_text = f\"User: {prompt}\\nAssistant:\"\n",
    "        inputs = trainer.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        if config.use_gpu and torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = trainer.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=trainer.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode the response\n",
    "        full_response = trainer.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the assistant's response\n",
    "        if \"Assistant:\" in full_response:\n",
    "            response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "        else:\n",
    "            response = full_response\n",
    "        \n",
    "        print(f\"üéØ Response: {response}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating response: {e}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Test with some example prompts\n",
    "test_prompts = [\n",
    "    \"How do I monitor GPU usage on my system?\",\n",
    "    \"I need to find all Python files in my project\",\n",
    "    \"What's the best way to backup my database?\",\n",
    "    \"Help me set up a development environment for machine learning\",\n",
    "    \"How do I check which processes are using the most memory?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing model with sample prompts...\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüîç Test {i}/5:\")\n",
    "    test_model_interactive(prompt, max_tokens=150, temperature=0.7)\n",
    "    \n",
    "print(\"\\n‚úÖ Interactive testing completed!\")\n",
    "print(\"\\nüí° Tip: You can call test_model_interactive('your prompt here') to test with custom prompts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b5145",
   "metadata": {},
   "source": [
    "## 7. Model Saving and Deployment\n",
    "\n",
    "Let's save our trained model and prepare it for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and tokenizer\n",
    "print(\"üíæ Saving trained model...\")\n",
    "\n",
    "try:\n",
    "    # Save model and tokenizer\n",
    "    trainer.model.save_pretrained(config.output_dir)\n",
    "    trainer.tokenizer.save_pretrained(config.output_dir)\n",
    "    \n",
    "    # Save configuration\n",
    "    import json\n",
    "    config_dict = {\n",
    "        'base_model': config.base_model,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'batch_size': config.batch_size,\n",
    "        'num_epochs': config.num_epochs,\n",
    "        'max_sequence_length': config.max_sequence_length,\n",
    "        'training_completed': datetime.now().isoformat(),\n",
    "        'total_training_examples': len(train_data),\n",
    "        'model_parameters': trainer.model.num_parameters()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(config.output_dir, 'training_config.json'), 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved successfully to: {config.output_dir}\")\n",
    "    print(f\"üìÅ Saved files:\")\n",
    "    saved_files = os.listdir(config.output_dir)\n",
    "    for file in saved_files:\n",
    "        print(f\"   - {file}\")\n",
    "    \n",
    "    # Calculate model size\n",
    "    model_size = sum(os.path.getsize(os.path.join(config.output_dir, f)) \n",
    "                    for f in saved_files if os.path.isfile(os.path.join(config.output_dir, f)))\n",
    "    print(f\"üìè Total model size: {model_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n",
    "\n",
    "# Create deployment summary\n",
    "print(f\"\\nüöÄ Deployment Information:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model Location: {config.output_dir}\")\n",
    "print(f\"Base Model: {config.base_model}\")\n",
    "print(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Parameters: {trainer.model.num_parameters():,}\")\n",
    "print(f\"Training Examples: {len(train_data)}\")\n",
    "print(f\"Validation Examples: {len(val_data)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìã To use this model in production:\")\n",
    "print(f\"```python\")\n",
    "print(f\"from transformers import AutoTokenizer, AutoModelForCausalLM\")\n",
    "print(f\"\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{config.output_dir}')\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{config.output_dir}')\")\n",
    "print(f\"```\")\n",
    "\n",
    "# Integration with continuous learning\n",
    "print(f\"\\nüîÑ Setting up continuous learning...\")\n",
    "try:\n",
    "    learning_manager = ContinuousLearningManager()\n",
    "    print(\"‚úÖ Continuous learning manager initialized\")\n",
    "    print(\"üìù User interactions will be logged for future training iterations\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not initialize continuous learning: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Training pipeline completed successfully!\")\n",
    "print(f\"üìä Summary:\")\n",
    "print(f\"   - Model: Gemma 3n fine-tuned for system administration\")\n",
    "print(f\"   - Training time: {training_time/60:.1f} minutes\")\n",
    "print(f\"   - Examples processed: {len(train_data)}\")\n",
    "print(f\"   - Model saved to: {config.output_dir}\")\n",
    "print(f\"   - Ready for deployment!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17006ee9",
   "metadata": {},
   "source": [
    "## 8. Next Steps and Usage\n",
    "\n",
    "üéâ **Congratulations!** You've successfully trained your Overseer Gemma 3n model.\n",
    "\n",
    "### What you accomplished:\n",
    "- ‚úÖ Loaded and configured the Gemma 3n base model\n",
    "- ‚úÖ Prepared training data from multiple sources\n",
    "- ‚úÖ Fine-tuned the model on system administration tasks\n",
    "- ‚úÖ Evaluated model performance\n",
    "- ‚úÖ Saved the trained model for deployment\n",
    "\n",
    "### Next steps:\n",
    "1. **Integration**: Integrate this model into the Overseer backend system\n",
    "2. **Testing**: Run more comprehensive evaluation on real-world tasks\n",
    "3. **Monitoring**: Set up continuous learning to improve the model over time\n",
    "4. **Optimization**: Consider model quantization for faster inference\n",
    "5. **Deployment**: Deploy to production environment with proper monitoring\n",
    "\n",
    "### Usage in production:\n",
    "```python\n",
    "# Load your trained model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"your_output_directory\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Generate responses\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(f\"User: {prompt}\\nAssistant:\", return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "```\n",
    "\n",
    "### Continuous improvement:\n",
    "- Monitor user interactions and feedback\n",
    "- Periodically retrain with new data\n",
    "- A/B test different model versions\n",
    "- Track performance metrics in production"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
