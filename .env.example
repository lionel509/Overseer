# Overseer AI Training Configuration Example
# Copy this file to .env and configure your specific values

# Model Configuration
MODEL_NAME=gemma3n:latest
HUGGING_FACE_MODEL=google/gemma-3n-9b-it
BASE_MODEL_PATH=./models/base_gemma
FINE_TUNED_MODEL_PATH=./models/overseer_gemma

# Training Configuration
OUTPUT_DIR=./outputs/overseer_gemma_lora
NUM_TRAIN_EPOCHS=3
PER_DEVICE_TRAIN_BATCH_SIZE=1
PER_DEVICE_EVAL_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=8
WARMUP_STEPS=50
LEARNING_RATE=2e-4
WEIGHT_DECAY=0.01
LOGGING_STEPS=10
SAVE_STEPS=100
EVAL_STEPS=100
MAX_GRAD_NORM=1.0
MAX_STEPS=500
DATALOADER_NUM_WORKERS=4

# LoRA Configuration
LORA_RANK=16
LORA_ALPHA=32
LORA_DROPOUT=0.1

# Dataset Configuration
KAGGLE_DATASET_IT_HELPDESK=amandalund/it-help-desk-tickets
KAGGLE_DATASET_CUSTOMER_SUPPORT=thoughtvector/customer-support-on-twitter
KAGGLE_DATASET_NETFLIX_SHOWS=shivamb/netflix-shows
KAGGLE_DATASET_ARXIV=Cornell-University/arxiv
KAGGLE_USERNAME=your_kaggle_username
KAGGLE_KEY=your_kaggle_key

# Data Preprocessing
MAX_LENGTH=512
TEST_SIZE=0.2
VAL_SIZE=0.1

# Deployment Configuration
EXPORT_PATH=./models/overseer_deployed
OLLAMA_MODEL_NAME=overseer-gemma

# Evaluation Configuration
MAX_NEW_TOKENS=150
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=40
REPEAT_PENALTY=1.1
NUM_CTX=2048

# System Configuration
RANDOM_SEED=42
LOG_LEVEL=INFO

# Kaggle API Configuration (Optional)
# KAGGLE_USERNAME=your_kaggle_username
# KAGGLE_KEY=your_kaggle_api_key

# Weights & Biases Configuration (Optional)
# WANDB_PROJECT=overseer-training
# WANDB_API_KEY=your_wandb_api_key

# Hugging Face Configuration (Optional)
# HF_TOKEN=your_huggingface_token
